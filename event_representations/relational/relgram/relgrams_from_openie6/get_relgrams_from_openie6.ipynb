{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf3806b",
   "metadata": {},
   "source": [
    "# Get relgrams from openie6. (v3)\n",
    "26.04.2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70485062",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "libcudnn.so.7: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m displacy\n\u001b[1;32m      7\u001b[0m spacy\u001b[38;5;241m.\u001b[39mprefer_gpu()\n",
      "File \u001b[0;32m/mount/arbeitsdaten31/studenten1/shencg/BA/lib64/python3.10/site-packages/spacy/__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prefer_gpu, require_gpu, require_cpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m/mount/arbeitsdaten31/studenten1/shencg/BA/lib64/python3.10/site-packages/thinc/api.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, registry, ConfigValidationError\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minitializers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normal_init, uniform_init, glorot_uniform_init, zero_init\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minitializers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m configure_normal_init\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CategoricalCrossentropy, L2Distance, CosineDistance\n",
      "File \u001b[0;32m/mount/arbeitsdaten31/studenten1/shencg/BA/lib64/python3.10/site-packages/thinc/initializers.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, cast\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ops\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FloatsXd, Shape\n",
      "File \u001b[0;32m/mount/arbeitsdaten31/studenten1/shencg/BA/lib64/python3.10/site-packages/thinc/backends/__init__.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcontextvars\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ContextVar\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ops\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcupy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CupyOps, has_cupy\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n",
      "File \u001b[0;32m/mount/arbeitsdaten31/studenten1/shencg/BA/lib64/python3.10/site-packages/thinc/backends/ops.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FloatsXd, Ints1d, Ints2d, Ints3d, Ints4d, IntsXd, _Floats\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeviceTypes, Generator, Padded, Batchable, SizedGenerator\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_array_module, is_xp_array, to_numpy\n\u001b[1;32m     16\u001b[0m ArrayT \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArrayT\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39mArrayXd)\n\u001b[1;32m     17\u001b[0m FloatsT \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFloatsT\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39m_Floats)\n",
      "File \u001b[0;32m/mount/arbeitsdaten31/studenten1/shencg/BA/lib64/python3.10/site-packages/thinc/util.py:57\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m     has_tensorflow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmxnet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmx\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     has_mxnet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[0;32m/mount/arbeitsdaten31/studenten1/shencg/BA/lib/python3.10/site-packages/mxnet/__init__.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed to the Apache Software Foundation (ASF) under one\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# coding: utf-8\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"MXNet: a concise, fast and flexible framework for deep learning.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Context, current_context, cpu, gpu, cpu_pinned\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m engine, error\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MXNetError\n",
      "File \u001b[0;32m/mount/arbeitsdaten31/studenten1/shencg/BA/lib/python3.10/site-packages/mxnet/context.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mctypes\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classproperty, with_metaclass, _MXClassPropertyMetaClass\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _LIB\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_call\n",
      "File \u001b[0;32m/mount/arbeitsdaten31/studenten1/shencg/BA/lib/python3.10/site-packages/mxnet/base.py:356\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    354\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m libinfo\u001b[38;5;241m.\u001b[39m__version__\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# library instance of mxnet\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m _LIB \u001b[38;5;241m=\u001b[39m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# type definitions\u001b[39;00m\n\u001b[1;32m    359\u001b[0m mx_int \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int\n",
      "File \u001b[0;32m/mount/arbeitsdaten31/studenten1/shencg/BA/lib/python3.10/site-packages/mxnet/base.py:347\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    345\u001b[0m     lib \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mCDLL(lib_path[\u001b[38;5;241m0\u001b[39m], winmode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0x00000008\u001b[39m)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     lib \u001b[38;5;241m=\u001b[39m \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib_path\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRTLD_LOCAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# DMatrix functions\u001b[39;00m\n\u001b[1;32m    349\u001b[0m lib\u001b[38;5;241m.\u001b[39mMXGetLastError\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_char_p\n",
      "File \u001b[0;32m/usr/lib64/python3.10/ctypes/__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: libcudnn.so.7: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import dill as pickle\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "openie6_path = \"/mount/studenten/arbeitsdaten-studenten1/shencg/BA/models/event_extraction/relational/openie6/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5de1b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478a70d7",
   "metadata": {},
   "source": [
    "### Get relgrams for goal sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55483f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_ids(relgram, sentence, nlp):\n",
    "    \"\"\"\n",
    "    Get the index of each word in the relgram.\n",
    "    If the relgram contains multiple words, aggregate the index of each word in a list.\n",
    "    For example, for the sentence \"People hang an ironing board\" and relgram [\"People\", \"hang\", \"an ironing board\"], \n",
    "    the words_ids = [[0], [1], [2, 3, 4]].\n",
    "    \"\"\"\n",
    "    \n",
    "    word_ids = []\n",
    "    \n",
    "    doc_sentence = nlp(sentence)\n",
    "    sentence_words = [str(token) for token in doc_sentence]\n",
    "#     print(\"SENTENCE_WORDS:\", sentence_words)\n",
    "    \n",
    "    wid2word = dict(zip(range(len(sentence_words)), sentence_words))\n",
    "#     print(\"wid2word:\", wid2word, '\\n')\n",
    "    \n",
    "    relgram_words = [string.split() for string in relgram]\n",
    "#     print(\"RELGRAM in get_word_ids:\", relgram)\n",
    "#     print(\"RELGRAM_WORDS:\", relgram_words)\n",
    "    for words in relgram_words:\n",
    "#         print(\"WORDS:\", words)\n",
    "        if len(words) == 1:\n",
    "            if words[0] == \"[PAD]\":\n",
    "                word_ids.append([-1])\n",
    "            else:\n",
    "                if words[0] in list(wid2word.values()):\n",
    "                    wid = list(wid2word.keys())[list(wid2word.values()).index(words[0])]\n",
    "                    word_ids.append([wid])\n",
    "    #                 del wid2word[wid]\n",
    "                    wid2word[wid] = \"\"\n",
    "        elif len(words) > 1:\n",
    "            word_ids_tmp = []\n",
    "            for word in words:\n",
    "#                 print(\"word:\", word)\n",
    "                if word in list(wid2word.values()):\n",
    "#                     print(list(wid2word.values()).index(word))\n",
    "                    wid = list(wid2word.keys())[list(wid2word.values()).index(word)]\n",
    "                    word_ids_tmp.append(wid)\n",
    "    #                 del wid2word[wid]\n",
    "                    wid2word[wid] = \"\"\n",
    "            word_ids.append(word_ids_tmp)\n",
    "        else:\n",
    "#             assert False\n",
    "            continue\n",
    "            \n",
    "    return word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relgrams(predictions):\n",
    "    fid2predictions = defaultdict()\n",
    "    for i, item in enumerate(tqdm(predictions)):\n",
    "        relgrams = []\n",
    "        word_ids_list = []\n",
    "        if item.startswith(\"fid:\"):\n",
    "            fid = item[4:-1]\n",
    "            sentence = predictions[i+1].split(\"\\n\")[0][:-2]\n",
    "            prediction = predictions[i+1].split('\\n')[1:]\n",
    "            for item in prediction:\n",
    "                if len(item) > 0:\n",
    "                    prediction = item[6:][1:-1]\n",
    "                    if prediction.startswith(\"People\"):\n",
    "                        relgram = prediction.split(\"; \")\n",
    "                        relgrams.append(relgram)\n",
    "\n",
    "            for relgram in relgrams:\n",
    "                doc = nlp(\" \".join(relgram))\n",
    "                for token in doc:\n",
    "                    if token.dep_ == 'ROOT':\n",
    "                        i_root = token.i\n",
    "                    if token.dep_ == 'dobj' and token.head.i == i_root:\n",
    "                        i_dobj = token.i\n",
    "                        last_item = doc[i_root+1:i_dobj+1].text\n",
    "                        relgram[-1] = last_item\n",
    "                        break\n",
    "                    if token.dep_ == 'prep' and token.head.i == i_root:\n",
    "                        i_prep = token.i\n",
    "                        last_item = doc[i_root+1:i_prep].text\n",
    "                        if last_item == '':\n",
    "                            relgram[-1] = \"[PAD]\"\n",
    "                        else:\n",
    "                            relgram[-1] = last_item\n",
    "\n",
    "\n",
    "                # Get word_ids in each relgram.\n",
    "                word_ids = get_word_ids(relgram, sentence, nlp)\n",
    "                word_ids_list.append(word_ids)\n",
    "            \n",
    "            fid2predictions[fid] = [sentence, relgrams, word_ids_list]\n",
    "    \n",
    "    fid2predictions = dict(fid2predictions)\n",
    "    print(\"Number of relgrams:\", len(fid2predictions))   \n",
    "    \n",
    "    return fid2predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8be2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions of openIE 6.\n",
    "predictions = pickle.load(open(openie6_path + \"Experiment/output/goals/predictions.p\", \"rb\"))\n",
    "for pred in predictions:\n",
    "    if pred == \"\\n\":\n",
    "        predictions.remove(pred)\n",
    "print(\"Number of predictions:\", len(predictions), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16feacfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fid2predictions = get_relgrams(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10ef17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing.\n",
    "def post_processing(fid2predictions):\n",
    "    for fid, pred in fid2predictions.items():\n",
    "        for relgram, indices in zip(pred[1], pred[2]):\n",
    "            if relgram[2] == '':\n",
    "                print(relgram, indices)\n",
    "                if len(relgram[1].split()) == 1:\n",
    "                    doc = nlp(\" \".join([relgram[0], relgram[1]]))\n",
    "                    pos_list = [token.pos_ for token in doc]\n",
    "                    if pos_list[1] == 'VERB':\n",
    "                        relgram[2] = '[PAD]'\n",
    "                        indices += [[-1]]\n",
    "                    else:\n",
    "                        pred[1].remove(relgram)\n",
    "                        pred[2].remove(indices)\n",
    "                else:\n",
    "                    pred[1].remove(relgram)\n",
    "                    pred[2].remove(indices)\n",
    "\n",
    "        for relgram, indices in zip(pred[1], pred[2]):\n",
    "            if relgram[2] == '':\n",
    "                pred[1].remove(relgram)\n",
    "                pred[2].remove(indices)\n",
    "                \n",
    "    return fid2predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc839304",
   "metadata": {},
   "outputs": [],
   "source": [
    "fid2predictions = post_processing(fid2predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0171c9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write the post-processed file.\n",
    "# with open(\"fid2predictions.p\", \"wb\") as file:\n",
    "#     pickle.dump(fid2predictions, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f215bf35",
   "metadata": {},
   "source": [
    "### Get relgrams for step headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bbad73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b19f5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
